{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Description\n",
    "This notebook contains all code used in the creation of the MassBay_2017_to_2022.csv file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MWRA/MassBay Data Concatenation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import dateparser\n",
    "from pytz import timezone\n",
    "import shutil\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '../../data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OldFormat Concatenation (2017 - 2019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_data_dir = data_dir + 'MassBay/OldFormat/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load old format excel files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "whitelist = [ # Files that have more than one sheet but only first sheet is significant\n",
    "  'WN172 event dataMarch2017.xlsx',\n",
    " 'WN173 sensor dataApril2017.xlsx',\n",
    " 'WN177 Event dataAug2017.xlsx']\n",
    " \n",
    "# check that each excel file in the data_dir has only one sheet, add to blacklist if not\n",
    "csvs = {}\n",
    "for file in os.listdir(old_data_dir):\n",
    "    if file.endswith('.xlsx'):\n",
    "        all_sheets = pd.read_excel(old_data_dir + file, sheet_name=None)\n",
    "        if len(all_sheets) > 1 and file not in whitelist:\n",
    "          print('ERROR:', file, 'has', len(all_sheets), 'sheets')\n",
    "          continue\n",
    "        else:\n",
    "          first_sheet = all_sheets[list(all_sheets.keys())[0]]\n",
    "          csvs[file] = first_sheet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correct column inconsistencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in common: {'Longitude', 'Chla Fluor', 'DO % Saturation', 'SampleDateTime', 'Temperature', 'Salinity', 'pH <2>', 'SampleID', 'Sigma-T', 'Air Irradiance', 'Depth', 'StationID', 'Dissolved Oxygen (Model 43)', 'Water Irradiance', 'Beam Attenuation', 'Latitude', 'Conductivity'}\n"
     ]
    }
   ],
   "source": [
    "# Correct insignificant column inconsistency in June 2018 data\n",
    "june2018 = 'WN185 Event dataJune2018.xlsx'\n",
    "cols_to_drop = []\n",
    "columns = set(list(csvs.values())[0].columns)\n",
    "for col in csvs[june2018].columns:\n",
    "    if col not in columns:\n",
    "        cols_to_drop.append(col)\n",
    "csvs[june2018].drop(columns = cols_to_drop, inplace = True)\n",
    "\n",
    "# Correct inisgnificant column inconsistency in Feb 2017 data\n",
    "csvs['WN171 Event dataFeb2017.xlsx'].drop(columns='Altitude', inplace=True)\n",
    "\n",
    "# check that all elements of csvs have the same columns\n",
    "columns = set(list(csvs.values())[0].columns)\n",
    "print(f'Columns in common: {columns}')\n",
    "for item in csvs.items():\n",
    "  if columns != set(item[1].columns):\n",
    "        print(\"ERROR: Column inconsistency in :\" +item[0]) # error  if not all elements of csvs have the same columns (should be the same)\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Concatenate the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate all csvs into one dataframe and save as csv\n",
    "csv_list = list(csvs.values())\n",
    "old_format_concat = pd.concat(csv_list, ignore_index=True, sort=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Post-process concatenated dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add data source column\n",
    "old_format_concat['Data Source'] = 'MassBay_2017_to_2019_CONCAT'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Export as csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save as csv\n",
    "old_format_concat.to_csv(data_dir + '/MassBay/OldFormat/concat/MassBay_2017_to_2019_CONCAT.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New Format Concatenation (2020-)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data_dir = data_dir + 'MassBay/NewFormat/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load new format excel files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "whitelist = []\n",
    "# check that each excel file in the data_dir has only one sheet, add to blacklist if not\n",
    "newcsvs = {}\n",
    "for file in os.listdir(new_data_dir):\n",
    "    if file.endswith('.xlsx'):\n",
    "        all_sheets = pd.read_excel(new_data_dir + file, sheet_name=None)\n",
    "        if len(all_sheets) != 1 and file not in whitelist:\n",
    "            print('ERROR:', file, 'has', len(all_sheets), 'sheets')\n",
    "        else:\n",
    "            first_sheet = all_sheets[list(all_sheets.keys())[0]]\n",
    "            newcsvs[file] = first_sheet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correct column inconsistencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correct column mapping inconsistencies in new format\n",
    "# Drop inconsistent columns\n",
    "for csv in newcsvs:\n",
    "    if 'DO_RAW (mg/L)' in newcsvs[csv].columns:\n",
    "        newcsvs[csv]['DISS_OXYGEN (mg/L)'] = newcsvs[csv]['DO_RAW (mg/L)']\n",
    "        newcsvs[csv].drop(columns = ['DO_RAW (mg/L)'], inplace = True)\n",
    "    if 'PCT_SAT_RAW (PCT)' in newcsvs[csv].columns:\n",
    "        newcsvs[csv]['PCT_SAT (PCT)'] = newcsvs[csv]['PCT_SAT_RAW (PCT)']\n",
    "        newcsvs[csv].drop(columns = ['PCT_SAT_RAW (PCT)'], inplace = True)\n",
    "        \n",
    "# Remove extraneous columns in March 2022 data\n",
    "newcsvs[\"WN222_Event_dataMar2022.xlsx\"].drop(columns = ['Unnamed: 16',\n",
    "       'Unnamed: 17', 'Unnamed: 18', 'Unnamed: 19', 'Unnamed: 20',\n",
    "       'Unnamed: 21', 'Unnamed: 22', 'Unnamed: 23', 'Unnamed: 24',\n",
    "       'Unnamed: 25', 'Unnamed: 26', 'Unnamed: 27', 'Unnamed: 28',\n",
    "       'Unnamed: 29', 'Unnamed: 30', 'Unnamed: 31', 'Unnamed: 32',\n",
    "       'Unnamed: 33', 'Unnamed: 34', 'Unnamed: 35'], inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Concatenate files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate\n",
    "newconcat = pd.concat(newcsvs, ignore_index=True, sort=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Post-process concatenated dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/becklabash/opt/anaconda3/lib/python3.7/site-packages/dateparser/date_parser.py:35: PytzUsageWarning: The localize method is no longer necessary, as this time zone supports the fold attribute (PEP 495). For more details on migrating to a PEP 495-compliant implementation, see https://pytz-deprecation-shim.readthedocs.io/en/latest/migration.html\n",
      "  date_obj = stz.localize(date_obj)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove exraneous colulms from concat\n",
    "newconcat = newconcat.drop(columns = ['COMMENTS'])\n",
    "\n",
    "# Add data source column\n",
    "newconcat['Data Source'] = 'MassBay_2020_to_2022_CONCAT'\n",
    "\n",
    "# Remove space from station col\n",
    "newconcat['STAT_ID'] = [stat.strip(\" \") for stat in newconcat['STAT_ID']]\n",
    "\n",
    "# Convert timestamp to datetime if string\n",
    "for ind, row in newconcat.iterrows():\n",
    "    date_str = row['PROF_DATE_TIME_LOCAL']\n",
    "    if type(date_str) == str:\n",
    "        newconcat.loc[ind, 'PROF_DATE_TIME_LOCAL'] = dateparser.parse(date_str)\n",
    "\n",
    "len(newconcat) == 1257"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Export as csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save concat\n",
    "newconcat.to_csv(data_dir + 'MassBay/NewFormat/concat/MassBay_2020_to_2022_CONCAT.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging Old and New Formats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the old and new format concatenations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_format = pd.read_csv(data_dir + 'MassBay/OldFormat/concat/MassBay_2017_to_2019_CONCAT.csv')\n",
    "new_format = pd.read_csv(data_dir + 'MassBay/NewFormat/concat/MassBay_2020_to_2022_CONCAT.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Map the new colums to the old columns for concatenation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New:Old Column Mapping (retain new column names)\n",
    "new_to_old = {\n",
    "  'EVENT_ID' : 'SampleID',\n",
    " 'STAT_ID' : 'StationID',\n",
    " 'PROF_DATE_TIME_LOCAL' : 'SampleDateTime',\n",
    " 'LATITUDE' : 'Latitude',\n",
    " 'LONGITUDE' : 'Longitude',\n",
    " 'DEPTH (m)': 'Depth',\n",
    " 'CONDTVY (mS/cm)': 'Conductivity',\n",
    "'DISS_OXYGEN (mg/L)' : 'Dissolved Oxygen (Model 43)',\n",
    "'FLU_RAW (ug/L)' : 'Chla Fluor',\n",
    "'PCT_SAT (PCT)' : 'DO % Saturation',\n",
    "'pH ()': 'pH <2>',\n",
    " 'SAL (PSU)' : 'Salinity',\n",
    "'SIGMA_T ()': 'Sigma-T',\n",
    " 'TEMP (C)' : 'Temperature',\n",
    "'TRANS (m-1)' : 'Beam Attenuation'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_converted = old_format.copy()\n",
    "for new_col in new_to_old:\n",
    "    old_converted[new_col] = old_converted[new_to_old[new_col]]\n",
    "    old_converted = old_converted.drop(columns = [new_to_old[new_col]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Concatenate the formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "oldAndNew = pd.concat([new_format, old_converted], ignore_index=True, sort=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Post-process the concatenated dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "oldAndNew['Data Source'] = 'MassBay_2017_to_2022'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check dates correct\n",
    "for ind, row in oldAndNew.iterrows():\n",
    "    date = dateparser.parse(row['PROF_DATE_TIME_LOCAL'])\n",
    "    try:\n",
    "        x = date.strftime(\"%Y\")\n",
    "        if not x in [\"2017\", \"2018\", \"2019\", \"2020\", \"2021\", \"2022\"]:\n",
    "            print(date_str)\n",
    "    except:\n",
    "        print(date_str)\n",
    "    oldAndNew.loc[ind, 'PROF_DATE_TIME_LOCAL'] = date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "eastern = timezone(\"US/Eastern\")\n",
    "# Drop microseconds and set timezone\n",
    "oldAndNew['PROF_DATE_TIME_LOCAL'] = [date.replace(microsecond=0).replace(tzinfo=eastern) for date in oldAndNew['PROF_DATE_TIME_LOCAL']]\n",
    "\n",
    "#Convert to excel parsable string\n",
    "oldAndNew['PROF_DATE_TIME_LOCAL'] = [date.strftime(\"%Y-%m-%d %H:%M:%S%z\") for date in oldAndNew['PROF_DATE_TIME_LOCAL']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Export as csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "oldAndNew.to_csv(data_dir + '/MassBay/concat/MassBay_2017_to_2022.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_column_descriptions(df):\n",
    "    ind = 1\n",
    "    for col in df.columns:\n",
    "        print(f'Column {ind}: {col} is COLUMN DESCRIPTION   BAD FLAG : -1.E+34')\n",
    "        ind += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
